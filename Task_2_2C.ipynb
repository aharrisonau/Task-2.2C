{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 2.2C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aharrisonau/Task-2.2C/blob/main/Task_2_2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zla7Z6gpZErr"
      },
      "source": [
        "# **Task 2.2C**\n",
        "\n",
        "This file is based on Practical 2, modified to complete task 2.2C.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFxKS7KnwEG"
      },
      "source": [
        "# **Environment Import and Setup**\n",
        "\n",
        "We use a maze as the environment under consideration and proceed to install the required system dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WAiOX9DWZHfL",
        "outputId": "4fb7445a-2a18-4d24-c203-a5aca958bba0"
      },
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils  \n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install PyOpenGL==3.1.* \\\n",
        "            PyOpenGL-accelerate==3.1.* \\\n",
        "            gym[box2d]==0.17.* \n",
        "!pip install pyglet\n",
        "!pip install ffmpeg\n",
        "! pip install pyvirtualdisplay\n",
        "!pip install Image\n",
        "!pip install gym-maze-trustycoder83"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 993 kB in 1s (792 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/3c/f42a62b7784c04b20f8b88d6c8ad04f4f20b0767b721102418aad94d8389/PyOpenGL-accelerate-3.1.5.tar.gz (538kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599199 sha256=777c5da692cbacd1f5faf5c43528f9ce4e23927ae2a50dcd34a0d3be7f0ed5d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/21/77/99670ceca25fddb3c2b60a7ae44644b8253d1006e8ec417bcc\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: PyOpenGL-accelerate, box2d-py\n",
            "Successfully installed PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n",
            "Collecting ffmpeg\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-cp37-none-any.whl size=6083 sha256=3c787cf826cad4309935de7185c7e83576906eaf8f3c35cf431a06a698432293\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n",
            "Collecting Image\n",
            "  Downloading https://files.pythonhosted.org/packages/84/be/961693ed384aa91bcc07525c90e3a34bc06c75f131655dfe21310234c933/image-1.5.33.tar.gz\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from Image) (7.1.2)\n",
            "Collecting django\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/6f/9a4415cc4fe9228e26ea53cf2005961799b2abb8da0411e519fdb74754fa/Django-3.1.7-py3-none-any.whl (7.8MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Image) (1.15.0)\n",
            "Collecting asgiref<4,>=3.2.10\n",
            "  Downloading https://files.pythonhosted.org/packages/89/49/5531992efc62f9c6d08a7199dc31176c8c60f7b2548c6ef245f96f29d0d9/asgiref-3.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from django->Image) (0.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from django->Image) (2018.9)\n",
            "Building wheels for collected packages: Image\n",
            "  Building wheel for Image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=7671a7dad5fe09daab1e4bb42c7a4fbd2bcdb8033133b2725e7dff04ec301ff0\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/4c/60/d5904e69c837fcdea7e03ffa0c657f35ced7e398c6f3ca17cc\n",
            "Successfully built Image\n",
            "Installing collected packages: asgiref, django, Image\n",
            "Successfully installed Image-1.5.33 asgiref-3.3.1 django-3.1.7\n",
            "Collecting gym-maze-trustycoder83\n",
            "  Downloading https://files.pythonhosted.org/packages/75/e0/e8da522c52d9697ed8eb612303e8f3cef25064e46dfd33c605ea5a0f0cb5/gym_maze_trustycoder83-0.0.4-py2.py3-none-any.whl\n",
            "Collecting pygame==1.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b3/0bf5afdcf6ef95d2a343cd7865585a6efe5e3e727c1a4f3385c9935248cf/pygame-1.9.6-cp37-cp37m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 6.1MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 6.7MB/s \n",
            "\u001b[?25hCollecting gym==0.17.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/99/7cc3e510678119cdac91f33fb9235b98448f09a6bdf0cafea2b108d9ce51/gym-0.17.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->gym-maze-trustycoder83) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->gym-maze-trustycoder83) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->gym-maze-trustycoder83) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.2->gym-maze-trustycoder83) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.2-cp37-none-any.whl size=1650891 sha256=4c9b6ef7f5ec7fb62cd68e346fa1bb90ab45c795cbd3dbc2b3e4599a0e288076\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/e0/91/f56e44e8062f8cd549673da49f59e1d4fe8b17398119b1d221\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pygame, numpy, gym, gym-maze-trustycoder83\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.17.2 gym-maze-trustycoder83-0.0.4 numpy-1.18.5 pygame-1.9.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fvtyvc7iIG"
      },
      "source": [
        "If the directory vid exists and has videos left over from previous tries, its better to clean it up before continuing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYLhSf9jP15B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167b249d-6e4b-4aed-ce64-b674c2483a40"
      },
      "source": [
        "!mkdir ./vid\n",
        "!rm ./vid/*.*"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './vid/*.*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPAWLq32DLLe"
      },
      "source": [
        "We now proceed to initialise the monitor wrapper for Gym so we can visualise the maze and the agent on a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGCPcwYZD3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05412ba-ea2e-43fb-f5b5-f3196d361f65"
      },
      "source": [
        "import sys\n",
        "# import pygame\n",
        "import numpy as np\n",
        "# import math\n",
        "# import base64\n",
        "# import io\n",
        "# import IPython\n",
        "import gym\n",
        "import gym_maze\n",
        "\n",
        "# from gym.wrappers import Monitor\n",
        "# from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "# Recording filename\n",
        "video_name = \"./vid/Practical_2.mp4\"\n",
        "\n",
        "# Setup the environment for the maze\n",
        "env = gym.make(\"maze-sample-10x10-v0\")\n",
        "\n",
        "##### Disabled all video to speed up execution ######\n",
        "# Setup the video\n",
        "#vid = None\n",
        "#vid = video_recorder.VideoRecorder(env,video_name)\n",
        "\n",
        "# env = gym.wrappers.Monitor(env,'./vid',force=True)\n",
        "current_state = env.reset()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4j3udI4Dbs6"
      },
      "source": [
        "The code below has been updated to implement task 2.2C\n",
        "\n",
        "Modified sections are indicated.\n",
        "\n",
        "As an initial check of the gridsearch and for comparison, the original model from practical 2 was run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Iw8fBRh4GD",
        "outputId": "a770ddb0-8735-4a08-f958-12651ecf9513"
      },
      "source": [
        "# original model run with the grid search.\n",
        "# as gamma was not used in the original model, it will be used to \n",
        "# vary the learning rate\n",
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "# set a range of epsilons values to try for the e-greedy algorithm\n",
        "epsilons = [0.01,0.05,0.1,0.5]\n",
        "# epsilons = [0.1] # single value for testing\n",
        "\n",
        "# set a range of gamma values to use\n",
        "gammas = [0.01,0.05,0.1,0.5,1.0]\n",
        "# gammas = [0.1] # single value for testing\n",
        "\n",
        "# loop through the combinations of epsilon and gamma and collect\n",
        "# the reward curves for each\n",
        "rewards_per_combination = {} \n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "\n",
        "        ######## Task 2.2 modifications START #########\n",
        "        # Initialize the Q-table to a realistic initiation.  For almost all actions\n",
        "        # the reward will be -0.1 / 100 = -0.1e-3 for the 10x10 maze\n",
        "        # so set the initial Q_table to be this\n",
        "        Q_table = np.zeros((len(states_dic),n_actions))\n",
        "        ######## Task 2.2 modifications END #########\n",
        "\n",
        "        epsilon = epsilons[i]\n",
        "        gamma = gammas[j]\n",
        "        lr = gamma # learning rate for original Practical 2\n",
        "        print()\n",
        "        print('epsilon = ',epsilon,' gamma = ', gamma)\n",
        "        \n",
        "        # set an empty list to collect results\n",
        "        rewards_per_episode = list()\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for e in range(n_episodes):\n",
        "          print(e, end=\" \") # get a heartbeat to know we are moving\n",
        "    \n",
        "          # We are not done yet\n",
        "          done = False\n",
        "    \n",
        "          # Sum the rewards that the agent gets from the environment\n",
        "          total_episode_reward = 0\n",
        "\n",
        "          for n in range(max_iter_episode): \n",
        "\n",
        "            #env.unwrapped.render()\n",
        "            #vid.capture_frame()\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample() # with probability epsilon, explore\n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates])) # otherwise exploit\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "            next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            # Update our Q-table using the Q-learning iteration\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            alpha = 1 / (n+1) # loop index starts at 0, so add 1\n",
        "            Q_table[current_Q_table_coordinates, action] = (1-lr)*Q_table[current_Q_table_coordinates, action] + lr*(reward + max(Q_table[next_Q_table_coordinates,:]))         \n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "            # If the episode is finished, we leave the for loop\n",
        "            if done:\n",
        "              break\n",
        "            current_state = next_state\n",
        "######## Task 2.2 modifications START #########    \n",
        "          #Reset enviroment for next episode\n",
        "          current_state = env.reset()\n",
        "    \n",
        "          rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # save the rewards list for this epsilon-gamma combination\n",
        "        rewards_per_combination[i,j]=rewards_per_episode\n",
        "        # end of the episode loop (on e)\n",
        "    #end of the gammas loop (on j)\n",
        "# end of the epsilons loop (on i)\n",
        "######## Task 2.2 modifications END #########\n",
        "        # Save video episode and close\n",
        "#print(\"Video successfuly saved.\")\n",
        "#vid.close()\n",
        "#vid.enabled = False"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epsilon =  0.01  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LIPtmhElCz2"
      },
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "av_total_rewards = pd.DataFrame(columns=gammas)\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "      av_total_rewards.loc[epsilons[i],gammas[j]]=statistics.mean(rewards_per_combination[i,j])\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "reiqI0I5lC0E",
        "outputId": "ebcffe4c-3cd9-4f5e-f1bd-e20d57cb8c2a"
      },
      "source": [
        "# print out the average rewards for the original Practical 2\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "av_total_rewards"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.050</th>\n",
              "      <th>0.100</th>\n",
              "      <th>0.500</th>\n",
              "      <th>1.000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.010</th>\n",
              "      <td>-0.286</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.660</td>\n",
              "      <td>0.740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.050</th>\n",
              "      <td>-0.288</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.565</td>\n",
              "      <td>0.606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.100</th>\n",
              "      <td>-0.138</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.572</td>\n",
              "      <td>0.579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.500</th>\n",
              "      <td>-0.399</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.538</td>\n",
              "      <td>0.165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0.010 0.050 0.100 0.500 1.000\n",
              "0.010 -0.286 0.238 0.223 0.660 0.740\n",
              "0.050 -0.288 0.154 0.335 0.565 0.606\n",
              "0.100 -0.138 0.166 0.343 0.572 0.579\n",
              "0.500 -0.399 0.071 0.273 0.538 0.165"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkSJE1LkdsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6781ee1-5bb3-49fb-9dd0-9e93d796f1e6"
      },
      "source": [
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "# set a range of epsilons values to try for the e-greedy algorithm\n",
        "epsilons = [0.01,0.05,0.1,0.5]\n",
        "# epsilons = [0.1] # single value for testing\n",
        "\n",
        "# set a range of gamma values to use\n",
        "gammas = [0.01,0.05,0.1,0.5,1.0]\n",
        "# gammas = [0.1] # single value for testing\n",
        "\n",
        "# loop through the combinations of epsilon and gamma and collect\n",
        "# the reward curves for each\n",
        "rewards_per_combination = {} \n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "\n",
        "        ######## Task 2.2 modifications START #########\n",
        "        # Initialize the Q-table to a realistic initiation.  For almost all actions\n",
        "        # the reward will be -0.1 / 100 = -0.1e-3 for the 10x10 maze\n",
        "        # so set the initial Q_table to be this\n",
        "        Q_table = np.full((len(states_dic),n_actions),-0.1e-3)\n",
        "        ######## Task 2.2 modifications END #########\n",
        "\n",
        "        epsilon = epsilons[i]\n",
        "        gamma = gammas[j]\n",
        "        print()\n",
        "        print('epsilon = ',epsilon,' gamma = ', gamma)\n",
        "        \n",
        "        # set an empty list to collect results\n",
        "        rewards_per_episode = list()\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for e in range(n_episodes):\n",
        "          print(e, end=\" \") # get a heartbeat to know we are moving\n",
        "    \n",
        "          # We are not done yet\n",
        "          done = False\n",
        "    \n",
        "          # Sum the rewards that the agent gets from the environment\n",
        "          total_episode_reward = 0\n",
        "\n",
        "          for n in range(max_iter_episode): \n",
        "\n",
        "            #env.unwrapped.render()\n",
        "            #vid.capture_frame()\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample() # with probability epsilon, explore\n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates])) # otherwise exploit\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "            next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            # Update our Q-table using the Q-learning iteration\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            alpha = 1 / (n+1) # loop index starts at 0, so add 1\n",
        "            Q_table[current_Q_table_coordinates, action] = (Q_table[current_Q_table_coordinates, action] \n",
        "                                                        + alpha*(reward - gamma * Q_table[current_Q_table_coordinates, action]))\n",
        "            # this implements the required Q update strategy\n",
        "            # Q(n+1) = Q(n) + alpha * (R - gamma * Q(n))\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "            # If the episode is finished, we leave the for loop\n",
        "            if done:\n",
        "              break\n",
        "            current_state = next_state\n",
        "######## Task 2.2 modifications START #########    \n",
        "          #Reset enviroment for next episode\n",
        "          current_state = env.reset()\n",
        "    \n",
        "          rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # save the rewards list for this epsilon-gamma combination\n",
        "        rewards_per_combination[i,j]=rewards_per_episode\n",
        "        # end of the episode loop (on e)\n",
        "    #end of the gammas loop (on j)\n",
        "# end of the epsilons loop (on i)\n",
        "######## Task 2.2 modifications END #########\n",
        "        # Save video episode and close\n",
        "#print(\"Video successfuly saved.\")\n",
        "#vid.close()\n",
        "#vid.enabled = False"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epsilon =  0.01  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ijBYCo9rOC"
      },
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "av_total_rewards = pd.DataFrame(columns=gammas)\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "      av_total_rewards.loc[epsilons[i],gammas[j]]=statistics.mean(rewards_per_combination[i,j])\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "QioEOghvGJRG",
        "outputId": "d8ce58cf-7c97-48c5-aed3-a2f106da3687"
      },
      "source": [
        "# print out the average rewards for the realistic initialization\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "av_total_rewards"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.050</th>\n",
              "      <th>0.100</th>\n",
              "      <th>0.500</th>\n",
              "      <th>1.000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.010</th>\n",
              "      <td>-0.389</td>\n",
              "      <td>-0.267</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.377</td>\n",
              "      <td>-0.383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.050</th>\n",
              "      <td>-0.294</td>\n",
              "      <td>-0.389</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>-0.271</td>\n",
              "      <td>-0.398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.100</th>\n",
              "      <td>-0.381</td>\n",
              "      <td>-0.281</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.500</th>\n",
              "      <td>-0.391</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>-0.382</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0.010  0.050  0.100  0.500  1.000\n",
              "0.010 -0.389 -0.267 -0.500 -0.377 -0.383\n",
              "0.050 -0.294 -0.389 -0.385 -0.271 -0.398\n",
              "0.100 -0.381 -0.281 -0.277 -0.388 -0.500\n",
              "0.500 -0.391 -0.276 -0.373 -0.392 -0.382"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgoWact4MS09"
      },
      "source": [
        "Repeat the process with a optimistic initialization.\n",
        "\n",
        "The maximum reward is 1, so optimize with this value, expecting to achieve the maximum reward on every step, which is very optomistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMDj_Uq-MQ64",
        "outputId": "30f6a4aa-accc-4358-e5e5-0dc796c66ea9"
      },
      "source": [
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "# set a range of epsilons values to try for the e-greedy algorithm\n",
        "epsilons = [0.01,0.05,0.1,0.5]\n",
        "# epsilons = [0.1] # single value for testing\n",
        "\n",
        "# set a range of gamma values to use\n",
        "gammas = [0.01,0.05,0.1,0.5,1.0]\n",
        "# gammas = [0.1] # single value for testing\n",
        "\n",
        "# loop through the combinations of epsilon and gamma and collect\n",
        "# the reward curves for each\n",
        "rewards_per_combination = {} \n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "        # Initialize the Q-table to an optomistic initiation.\n",
        "        # The maximum reward will be 1\n",
        "        # so set the initial Q_table to be this\n",
        "        # for each combination\n",
        "        Q_table = np.full((len(states_dic),n_actions),1.0)\n",
        "        \n",
        "\n",
        "        epsilon = epsilons[i]\n",
        "        gamma = gammas[j]\n",
        "        print()\n",
        "        print('epsilon = ',epsilon,' gamma = ', gamma)\n",
        "        \n",
        "        # set an empty list to collect results\n",
        "        rewards_per_episode = list()\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for e in range(n_episodes):\n",
        "          print(e, end=\" \") # get a heartbeat to know we are moving\n",
        "    \n",
        "          # We are not done yet\n",
        "          done = False\n",
        "    \n",
        "          # Sum the rewards that the agent gets from the environment\n",
        "          total_episode_reward = 0\n",
        "\n",
        "          for n in range(max_iter_episode): \n",
        "\n",
        "            #env.unwrapped.render()\n",
        "            #vid.capture_frame()\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample() # with probability epsilon, explore\n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates])) # otherwise exploit\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "            next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            # Update our Q-table using the Q-learning iteration\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            alpha = 1 / (n+1) # loop index starts at 0, so add 1\n",
        "            Q_table[current_Q_table_coordinates, action] = (Q_table[current_Q_table_coordinates, action] \n",
        "                                                        + alpha*(reward - gamma * Q_table[current_Q_table_coordinates, action]))\n",
        "            # this implements the required Q update strategy\n",
        "            # Q(n+1) = Q(n) + alpha * (R - gamma * Q(n))\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "            # If the episode is finished, we leave the for loop\n",
        "            if done:\n",
        "              break\n",
        "            current_state = next_state\n",
        "######## Task 2.2 modifications START #########    \n",
        "          #Reset enviroment for next episode\n",
        "          current_state = env.reset()\n",
        "    \n",
        "          rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # save the rewards list for this epsilon-gamma combination\n",
        "        rewards_per_combination[i,j]=rewards_per_episode\n",
        "        # end of the episode loop (on e)\n",
        "    #end of the gammas loop (on j)\n",
        "# end of the epsilons loop (on i)\n",
        "######## Task 2.2 modifications END #########\n",
        "        # Save video episode and close\n",
        "#print(\"Video successfuly saved.\")\n",
        "#vid.close()\n",
        "#vid.enabled = False"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epsilon =  0.01  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcnGHTuRNySy"
      },
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "av_total_rewards = pd.DataFrame(columns=gammas)\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "      av_total_rewards.loc[epsilons[i],gammas[j]]=statistics.mean(rewards_per_combination[i,j])\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "7_U1r-aUNyS-",
        "outputId": "5fd691da-2f99-46bb-9217-0130d8f41031"
      },
      "source": [
        "# print out the average rewards for the optomistic initialization\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "av_total_rewards"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.050</th>\n",
              "      <th>0.100</th>\n",
              "      <th>0.500</th>\n",
              "      <th>1.000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.010</th>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>-0.387</td>\n",
              "      <td>-0.394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.050</th>\n",
              "      <td>-0.287</td>\n",
              "      <td>-0.371</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.100</th>\n",
              "      <td>-0.270</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.378</td>\n",
              "      <td>-0.292</td>\n",
              "      <td>-0.394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.500</th>\n",
              "      <td>-0.277</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>-0.281</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0.010  0.050  0.100  0.500  1.000\n",
              "0.010 -0.388 -0.385 -0.291 -0.387 -0.394\n",
              "0.050 -0.287 -0.371 -0.390 -0.392 -0.500\n",
              "0.100 -0.270 -0.388 -0.378 -0.292 -0.394\n",
              "0.500 -0.277 -0.277 -0.281 -0.500 -0.283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}