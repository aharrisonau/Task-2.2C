{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 2.2C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aharrisonau/Task-2.2C/blob/main/Task_2_2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zla7Z6gpZErr"
      },
      "source": [
        "# **Task 2.2C**\n",
        "\n",
        "This file is based on Practical 2, modified to complete task 2.2C.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFxKS7KnwEG"
      },
      "source": [
        "# **Environment Import and Setup**\n",
        "\n",
        "We use a maze as the environment under consideration and proceed to install the required system dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAiOX9DWZHfL"
      },
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils  \n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install PyOpenGL==3.1.* \\\n",
        "            PyOpenGL-accelerate==3.1.* \\\n",
        "            gym[box2d]==0.17.* \n",
        "!pip install pyglet\n",
        "!pip install ffmpeg\n",
        "! pip install pyvirtualdisplay\n",
        "!pip install Image\n",
        "!pip install gym-maze-trustycoder83"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fvtyvc7iIG"
      },
      "source": [
        "If the directory vid exists and has videos left over from previous tries, its better to clean it up before continuing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYLhSf9jP15B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4493605a-fe0c-4ead-cec7-0166e995e887"
      },
      "source": [
        "!mkdir ./vid\n",
        "!rm ./vid/*.*"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./vid’: File exists\n",
            "rm: cannot remove './vid/*.*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPAWLq32DLLe"
      },
      "source": [
        "We now proceed to initialise the monitor wrapper for Gym so we can visualise the maze and the agent on a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGCPcwYZD3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea743f55-b40a-4cc8-8a1e-105064b1fba1"
      },
      "source": [
        "import sys\n",
        "# import pygame\n",
        "import numpy as np\n",
        "# import math\n",
        "# import base64\n",
        "# import io\n",
        "# import IPython\n",
        "import gym\n",
        "import gym_maze\n",
        "\n",
        "# from gym.wrappers import Monitor\n",
        "# from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "# Recording filename\n",
        "video_name = \"./vid/Practical_2.mp4\"\n",
        "\n",
        "# Setup the environment for the maze\n",
        "env = gym.make(\"maze-sample-10x10-v0\")\n",
        "\n",
        "##### Disabled all video to speed up execution ######\n",
        "# Setup the video\n",
        "#vid = None\n",
        "#vid = video_recorder.VideoRecorder(env,video_name)\n",
        "\n",
        "# env = gym.wrappers.Monitor(env,'./vid',force=True)\n",
        "current_state = env.reset()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4j3udI4Dbs6"
      },
      "source": [
        "The code below has been updated to implement task 2.2C\n",
        "\n",
        "Modified sections are indicated.\n",
        "\n",
        "As an initial check of the gridsearch and for comparison, the original model from practical 2 was run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Iw8fBRh4GD",
        "outputId": "fc1a2101-eed1-418a-dde6-1dea416ca0f1"
      },
      "source": [
        "# original model run with the grid search.\n",
        "# as gamma was not used in the original model, it will be used to \n",
        "# vary the learning rate\n",
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "# set a range of epsilons values to try for the e-greedy algorithm\n",
        "epsilons = [0.01,0.05,0.1,0.5]\n",
        "# epsilons = [0.1] # single value for testing\n",
        "\n",
        "# set a range of gamma values to use\n",
        "gammas = [0.01,0.05,0.1,0.5,1.0]\n",
        "# gammas = [0.1] # single value for testing\n",
        "\n",
        "# loop through the combinations of epsilon and gamma and collect\n",
        "# the reward curves for each\n",
        "rewards_per_combination = {} \n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "\n",
        "        ######## Task 2.2 modifications START #########\n",
        "        # Initialize the Q-table to a realistic initiation.  For almost all actions\n",
        "        # the reward will be -0.1 / 100 = -0.1e-3 for the 10x10 maze\n",
        "        # so set the initial Q_table to be this\n",
        "        Q_table = np.zeros((len(states_dic),n_actions))\n",
        "        ######## Task 2.2 modifications END #########\n",
        "\n",
        "        epsilon = epsilons[i]\n",
        "        gamma = gammas[j]\n",
        "        lr = gamma # learning rate for original Practical 2\n",
        "        print()\n",
        "        print('epsilon = ',epsilon,' gamma = ', gamma)\n",
        "        \n",
        "        # set an empty list to collect results\n",
        "        rewards_per_episode = list()\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for e in range(n_episodes):\n",
        "          print(e, end=\" \") # get a heartbeat to know we are moving\n",
        "    \n",
        "          # We are not done yet\n",
        "          done = False\n",
        "    \n",
        "          # Sum the rewards that the agent gets from the environment\n",
        "          total_episode_reward = 0\n",
        "\n",
        "          for n in range(max_iter_episode): \n",
        "\n",
        "            #env.unwrapped.render()\n",
        "            #vid.capture_frame()\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample() # with probability epsilon, explore\n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates])) # otherwise exploit\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "            next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            # Update our Q-table using the Q-learning iteration\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            alpha = 1 / (n+1) # loop index starts at 0, so add 1\n",
        "            Q_table[current_Q_table_coordinates, action] = (1-lr)*Q_table[current_Q_table_coordinates, action] + lr*(reward + max(Q_table[next_Q_table_coordinates,:]))         \n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "            # If the episode is finished, we leave the for loop\n",
        "            if done:\n",
        "              break\n",
        "            current_state = next_state\n",
        "######## Task 2.2 modifications START #########    \n",
        "          #Reset enviroment for next episode\n",
        "          current_state = env.reset()\n",
        "    \n",
        "          rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # save the rewards list for this epsilon-gamma combination\n",
        "        rewards_per_combination[i,j]=rewards_per_episode\n",
        "        # end of the episode loop (on e)\n",
        "    #end of the gammas loop (on j)\n",
        "# end of the epsilons loop (on i)\n",
        "######## Task 2.2 modifications END #########\n",
        "        # Save video episode and close\n",
        "#print(\"Video successfuly saved.\")\n",
        "#vid.close()\n",
        "#vid.enabled = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epsilon =  0.01  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LIPtmhElCz2"
      },
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "av_total_rewards = pd.DataFrame(columns=gammas)\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "      av_total_rewards.loc[epsilons[i],gammas[j]]=statistics.mean(rewards_per_combination[i,j])\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "reiqI0I5lC0E",
        "outputId": "97476eee-1c68-4081-b85d-14857c1d6137"
      },
      "source": [
        "# print out the average rewards for the original Practical 2\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "av_total_rewards"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.050</th>\n",
              "      <th>0.100</th>\n",
              "      <th>0.500</th>\n",
              "      <th>1.000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.010</th>\n",
              "      <td>-0.277</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.329</td>\n",
              "      <td>0.561</td>\n",
              "      <td>0.806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.050</th>\n",
              "      <td>-0.253</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.100</th>\n",
              "      <td>-0.285</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.500</th>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.044</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.384</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0.010  0.050 0.100 0.500 1.000\n",
              "0.010 -0.277  0.137 0.329 0.561 0.806\n",
              "0.050 -0.253  0.048 0.443 0.672 0.476\n",
              "0.100 -0.285  0.090 0.316 0.562 0.605\n",
              "0.500 -0.254 -0.044 0.177 0.527 0.384"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YIyjmvHVgFf"
      },
      "source": [
        "Now we can run the gridsearch routine, but using the alternative Q value update equation with a realistic Q value initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkSJE1LkdsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81adf5b9-23c5-4143-bb00-1193ae62ed11"
      },
      "source": [
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "# set a range of epsilons values to try for the e-greedy algorithm\n",
        "epsilons = [0.01,0.05,0.1,0.5]\n",
        "# epsilons = [0.1] # single value for testing\n",
        "\n",
        "# set a range of gamma values to use\n",
        "gammas = [0.01,0.05,0.1,0.5,1.0]\n",
        "# gammas = [0.1] # single value for testing\n",
        "\n",
        "# loop through the combinations of epsilon and gamma and collect\n",
        "# the reward curves for each\n",
        "rewards_per_combination = {} \n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "\n",
        "        ######## Task 2.2 modifications START #########\n",
        "        # Initialize the Q-table to a realistic initiation.  For almost all actions\n",
        "        # the reward will be -0.1 / 100 = -1e-3 for the 10x10 maze\n",
        "        # so set the initial Q_table to be this\n",
        "        Q_table = np.full((len(states_dic),n_actions),-1e-3)\n",
        "        \n",
        "        # Initialize the table of N values to store the number of times\n",
        "        # each action has been selected\n",
        "        N_table = np.zeros((len(states_dic),n_actions))\n",
        "        ######## Task 2.2 modifications END #########\n",
        "\n",
        "        epsilon = epsilons[i]\n",
        "        gamma = gammas[j]\n",
        "        print()\n",
        "        print('epsilon = ',epsilon,' gamma = ', gamma)\n",
        "        \n",
        "        # set an empty list to collect results\n",
        "        rewards_per_episode = list()\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for e in range(n_episodes):\n",
        "          print(e, end=\" \") # get a heartbeat to know we are moving\n",
        "    \n",
        "          # We are not done yet\n",
        "          done = False\n",
        "    \n",
        "          # Sum the rewards that the agent gets from the environment\n",
        "          total_episode_reward = 0\n",
        "\n",
        "          for n in range(max_iter_episode): \n",
        "\n",
        "            #env.unwrapped.render()\n",
        "            #vid.capture_frame()\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample() # with probability epsilon, explore\n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates])) # otherwise exploit\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "            next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            # Update our Q-table using the Q-learning iteration\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            if N_table[current_Q_table_coordinates, action] == 0:\n",
        "              alpha = 1\n",
        "            else:\n",
        "              alpha = 1 / N_table[current_Q_table_coordinates, action]\n",
        "            N_table[current_Q_table_coordinates, action] += 1\n",
        "            Q_table[current_Q_table_coordinates, action] = (Q_table[current_Q_table_coordinates, action] \n",
        "                                                        + alpha*(reward - gamma * Q_table[current_Q_table_coordinates, action]))\n",
        "            # this implements the required Q update strategy\n",
        "            # Q(n+1) = Q(n) + alpha * (R - gamma * Q(n))\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "            # If the episode is finished, we leave the for loop\n",
        "            if done:\n",
        "              break\n",
        "            current_state = next_state\n",
        "######## Task 2.2 modifications START #########    \n",
        "          #Reset enviroment for next episode\n",
        "          current_state = env.reset()\n",
        "    \n",
        "          rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # save the rewards list for this epsilon-gamma combination\n",
        "        rewards_per_combination[i,j]=rewards_per_episode\n",
        "        # end of the episode loop (on e)\n",
        "    #end of the gammas loop (on j)\n",
        "# end of the epsilons loop (on i)\n",
        "######## Task 2.2 modifications END #########\n",
        "        # Save video episode and close\n",
        "#print(\"Video successfuly saved.\")\n",
        "#vid.close()\n",
        "#vid.enabled = False"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epsilon =  0.01  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ijBYCo9rOC"
      },
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "av_total_rewards = pd.DataFrame(columns=gammas)\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "      av_total_rewards.loc[epsilons[i],gammas[j]]=statistics.mean(rewards_per_combination[i,j])\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "QioEOghvGJRG",
        "outputId": "41092f16-27db-4437-cbd9-247579245f51"
      },
      "source": [
        "# print out the average rewards for the realistic initialization\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "av_total_rewards"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.050</th>\n",
              "      <th>0.100</th>\n",
              "      <th>0.500</th>\n",
              "      <th>1.000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.010</th>\n",
              "      <td>-0.390</td>\n",
              "      <td>-0.297</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.395</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.050</th>\n",
              "      <td>-0.382</td>\n",
              "      <td>-0.393</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.100</th>\n",
              "      <td>-0.371</td>\n",
              "      <td>-0.376</td>\n",
              "      <td>-0.380</td>\n",
              "      <td>-0.361</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.500</th>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0.010  0.050  0.100  0.500  1.000\n",
              "0.010 -0.390 -0.297 -0.372 -0.395 -0.500\n",
              "0.050 -0.382 -0.393 -0.500 -0.392 -0.500\n",
              "0.100 -0.371 -0.376 -0.380 -0.361 -0.500\n",
              "0.500 -0.500 -0.275 -0.385 -0.388 -0.500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgoWact4MS09"
      },
      "source": [
        "Repeat the process with a optimistic initialization.\n",
        "\n",
        "The maximum reward is 1, so optimize with this value, expecting to achieve the maximum reward on every step, which is very optomistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMDj_Uq-MQ64",
        "outputId": "593d5ca1-ae6d-41d3-8fbe-9df4ba047317"
      },
      "source": [
        "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 500\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "# set a range of epsilons values to try for the e-greedy algorithm\n",
        "epsilons = [0.01,0.05,0.1,0.5]\n",
        "# epsilons = [0.1] # single value for testing\n",
        "\n",
        "# set a range of gamma values to use\n",
        "gammas = [0.01,0.05,0.1,0.5,1.0]\n",
        "# gammas = [0.1] # single value for testing\n",
        "\n",
        "# loop through the combinations of epsilon and gamma and collect\n",
        "# the reward curves for each\n",
        "rewards_per_combination = {} \n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "        # Initialize the Q-table to an optomistic initiation.\n",
        "        # The maximum reward will be 1\n",
        "        # so set the initial Q_table to be this\n",
        "        # for each combination\n",
        "        Q_table = np.full((len(states_dic),n_actions),1.0)\n",
        "        \n",
        "        # Initialize the table of N values to store the number of times\n",
        "        # each action has been selected\n",
        "        N_table = np.zeros((len(states_dic),n_actions))\n",
        "        \n",
        "\n",
        "        epsilon = epsilons[i]\n",
        "        gamma = gammas[j]\n",
        "        print()\n",
        "        print('epsilon = ',epsilon,' gamma = ', gamma)\n",
        "        \n",
        "        # set an empty list to collect results\n",
        "        rewards_per_episode = list()\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "        # Iterate over episodes\n",
        "        for e in range(n_episodes):\n",
        "          print(e, end=\" \") # get a heartbeat to know we are moving\n",
        "    \n",
        "          # We are not done yet\n",
        "          done = False\n",
        "    \n",
        "          # Sum the rewards that the agent gets from the environment\n",
        "          total_episode_reward = 0\n",
        "\n",
        "          for n in range(max_iter_episode): \n",
        "\n",
        "            #env.unwrapped.render()\n",
        "            #vid.capture_frame()\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample() # with probability epsilon, explore\n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates])) # otherwise exploit\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "            next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "######## Task 2.2 modifications START #########\n",
        "            # Update our Q-table using the Q-learning iteration\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            if N_table[current_Q_table_coordinates, action] == 0:\n",
        "              alpha = 1\n",
        "            else:\n",
        "              alpha = 1 / N_table[current_Q_table_coordinates, action]\n",
        "            N_table[current_Q_table_coordinates, action] += 1\n",
        "            Q_table[current_Q_table_coordinates, action] = (Q_table[current_Q_table_coordinates, action] \n",
        "                                                        + alpha*(reward - gamma * Q_table[current_Q_table_coordinates, action]))\n",
        "            # this implements the required Q update strategy\n",
        "            # Q(n+1) = Q(n) + alpha * (R - gamma * Q(n))\n",
        "######## Task 2.2 modifications END #########\n",
        "\n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "            # If the episode is finished, we leave the for loop\n",
        "            if done:\n",
        "              break\n",
        "            current_state = next_state\n",
        "######## Task 2.2 modifications START #########    \n",
        "          #Reset enviroment for next episode\n",
        "          current_state = env.reset()\n",
        "    \n",
        "          rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # save the rewards list for this epsilon-gamma combination\n",
        "        rewards_per_combination[i,j]=rewards_per_episode\n",
        "        # end of the episode loop (on e)\n",
        "    #end of the gammas loop (on j)\n",
        "# end of the epsilons loop (on i)\n",
        "######## Task 2.2 modifications END #########\n",
        "        # Save video episode and close\n",
        "#print(\"Video successfuly saved.\")\n",
        "#vid.close()\n",
        "#vid.enabled = False"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epsilon =  0.01  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.01  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.05  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.1  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.01\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.05\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.1\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  0.5\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "epsilon =  0.5  gamma =  1.0\n",
            "0 1 2 3 4 5 6 7 8 9 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcnGHTuRNySy"
      },
      "source": [
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "av_total_rewards = pd.DataFrame(columns=gammas)\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(gammas)):\n",
        "      av_total_rewards.loc[epsilons[i],gammas[j]]=statistics.mean(rewards_per_combination[i,j])\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "7_U1r-aUNyS-",
        "outputId": "105d622c-b7ec-4288-831e-742600712aee"
      },
      "source": [
        "# print out the average rewards for the optomistic initialization\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "av_total_rewards"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.050</th>\n",
              "      <th>0.100</th>\n",
              "      <th>0.500</th>\n",
              "      <th>1.000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.010</th>\n",
              "      <td>-0.267</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.050</th>\n",
              "      <td>-0.397</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.400</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.100</th>\n",
              "      <td>-0.361</td>\n",
              "      <td>-0.389</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.500</th>\n",
              "      <td>-0.399</td>\n",
              "      <td>-0.387</td>\n",
              "      <td>-0.375</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>-0.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0.010  0.050  0.100  0.500  1.000\n",
              "0.010 -0.267 -0.396 -0.372 -0.385 -0.500\n",
              "0.050 -0.397 -0.500 -0.400 -0.373 -0.500\n",
              "0.100 -0.361 -0.389 -0.500 -0.500 -0.500\n",
              "0.500 -0.399 -0.387 -0.375 -0.392 -0.500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}